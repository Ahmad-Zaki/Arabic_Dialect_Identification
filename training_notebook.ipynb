{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import ArabicTextNormalizer, preprocessing_pipeline, train_val_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from utils import confusion_matrix, report\n",
    "\n",
    "#Supress Warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"Datasets/dialect_dataset.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df[\"text\"], df[\"dialect\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_list = set(stopwords.words('arabic'))\n",
    "preprocessing = preprocessing_pipeline([\"normalization\",\"tfidf\"],\n",
    "                                       victorizer_kwarg = dict(ngram_range=(1, 5), min_df=10))\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pipeline',\n",
       "                 Pipeline(steps=[('arabictextnormalizer',\n",
       "                                  ArabicTextNormalizer()),\n",
       "                                 ('tfidfvectorizer',\n",
       "                                  TfidfVectorizer(min_df=10,\n",
       "                                                  ngram_range=(1, 5)))])),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(preprocessing, model)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = pipeline.predict(X_train)\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "y_test_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AE     0.5407    0.5012    0.5202     21036\n",
      "          BH     0.5156    0.4206    0.4633     21034\n",
      "          DZ     0.6807    0.5410    0.6028     12947\n",
      "          EG     0.6867    0.8941    0.7768     46108\n",
      "          IQ     0.7332    0.5640    0.6376     12397\n",
      "          JO     0.5747    0.4103    0.4788     22337\n",
      "          KW     0.5110    0.7140    0.5957     33687\n",
      "          LB     0.6824    0.7184    0.6999     22093\n",
      "          LY     0.6599    0.7438    0.6993     29199\n",
      "          MA     0.8379    0.5736    0.6810      9231\n",
      "          OM     0.5646    0.3987    0.4674     15293\n",
      "          PL     0.5122    0.6502    0.5730     34994\n",
      "          QA     0.5697    0.5704    0.5700     24855\n",
      "          SA     0.4971    0.5427    0.5189     21466\n",
      "          SD     0.8040    0.5741    0.6699     11548\n",
      "          SY     0.6565    0.3622    0.4669     12994\n",
      "          TN     0.7971    0.3834    0.5178      7397\n",
      "          YE     0.6738    0.1716    0.2736      7941\n",
      "\n",
      "    accuracy                         0.6027    366557\n",
      "   macro avg     0.6388    0.5408    0.5674    366557\n",
      "weighted avg     0.6121    0.6027    0.5932    366557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(\"Training\", y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AE     0.4230    0.3856    0.4034      2630\n",
      "          BH     0.3564    0.2765    0.3114      2629\n",
      "          DZ     0.5994    0.4734    0.5290      1618\n",
      "          EG     0.6396    0.8562    0.7322      5764\n",
      "          IQ     0.6637    0.4800    0.5571      1550\n",
      "          JO     0.4237    0.2915    0.3454      2792\n",
      "          KW     0.4214    0.6200    0.5018      4211\n",
      "          LB     0.6050    0.6365    0.6203      2762\n",
      "          LY     0.5933    0.6751    0.6316      3650\n",
      "          MA     0.8112    0.5546    0.6588      1154\n",
      "          OM     0.4323    0.2957    0.3511      1911\n",
      "          PL     0.4280    0.5578    0.4844      4374\n",
      "          QA     0.4463    0.4445    0.4454      3107\n",
      "          SA     0.3907    0.4078    0.3991      2683\n",
      "          SD     0.7490    0.5149    0.6103      1443\n",
      "          SY     0.4975    0.2438    0.3273      1624\n",
      "          TN     0.7366    0.3265    0.4524       925\n",
      "          YE     0.5369    0.1098    0.1823       993\n",
      "\n",
      "    accuracy                         0.5129     45820\n",
      "   macro avg     0.5419    0.4528    0.4746     45820\n",
      "weighted avg     0.5173    0.5129    0.5005     45820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(\"Validation\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred, y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AE     0.4283    0.3802    0.4028      2630\n",
      "          BH     0.3661    0.2864    0.3214      2629\n",
      "          DZ     0.5851    0.4376    0.5007      1618\n",
      "          EG     0.6504    0.8522    0.7378      5764\n",
      "          IQ     0.6775    0.4987    0.5745      1550\n",
      "          JO     0.4379    0.3044    0.3592      2792\n",
      "          KW     0.4177    0.6037    0.4938      4211\n",
      "          LB     0.6045    0.6459    0.6245      2762\n",
      "          LY     0.5952    0.6797    0.6347      3650\n",
      "          MA     0.7735    0.5208    0.6225      1154\n",
      "          OM     0.4402    0.2965    0.3544      1912\n",
      "          PL     0.4358    0.5649    0.4920      4374\n",
      "          QA     0.4537    0.4641    0.4589      3107\n",
      "          SA     0.3917    0.4372    0.4132      2683\n",
      "          SD     0.7652    0.5239    0.6220      1443\n",
      "          SY     0.5165    0.2691    0.3538      1624\n",
      "          TN     0.7488    0.3323    0.4603       924\n",
      "          YE     0.5482    0.1259    0.2048       993\n",
      "\n",
      "    accuracy                         0.5168     45820\n",
      "   macro avg     0.5465    0.4569    0.4795     45820\n",
      "weighted avg     0.5225    0.5168    0.5055     45820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(\"Testing\", y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred,\n",
    "                                        cmap = \"cividis\", \n",
    "                                        xticks_rotation = \"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000, split=\" \", oov_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_fit(X_train, y_train, X_val, y_val, model):\n",
    "  \"\"\"Preprocess training data and start the training process.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  X_train: Any\n",
    "    Training data text.\n",
    "  \n",
    "  y_train: Any\n",
    "    Training data labels.\n",
    "\n",
    "  X_val: Any\n",
    "    Validation data text.\n",
    "  \n",
    "  y_val: Any\n",
    "    Validation data labels.\n",
    "\n",
    "  model: Model\n",
    "    Keras model.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  history: keras.callbacks.History\n",
    "    Training history (loss, accuracy, val_loss, val_accuracy)\n",
    "  \"\"\"\n",
    "\n",
    "  normalizer = ArabicTextNormalizer()\n",
    "  X_train_norm = normalizer.transform(X_train)\n",
    "  X_val_norm = normalizer.transform(X_val)\n",
    "  tokenizer.fit_on_texts(X_train_norm)\n",
    "\n",
    "  X_train_tkn = tokenizer.texts_to_sequences(X_train_norm)\n",
    "  X_train_tkn = pad_sequences(X_train_tkn, maxlen=50)\n",
    "  X_val_tkn = tokenizer.texts_to_sequences(X_val_norm)\n",
    "  X_val_tkn = pad_sequences(X_val_tkn, maxlen=50)\n",
    "\n",
    "  y_train = pd.get_dummies(y_train).values\n",
    "  y_val = pd.get_dummies(y_val).values\n",
    "\n",
    "  es = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=1)\n",
    "  history = model.fit(X_train_tkn, y_train,\n",
    "                      validation_data=(X_val_tkn, y_val), \n",
    "                      epochs=10,\n",
    "                      batch_size=265,\n",
    "                      callbacks=es)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_dict = dict(zip([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 ,15 , 16, 17],\n",
    "                        [\"AE\",\"BH\",\"DZ\",\"EG\",\"IQ\",\"JO\",\"KW\",\"LB\",\"LY\",\"MA\",\"OM\",\"PL\",\"QA\",\"SA\",\"SD\",\"SY\",\"TN\",\"YE\"]))\n",
    "\n",
    "def dl_predict(X_test, model):\n",
    "  \"\"\"Preprocess training data and start the training process.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  X_test: Any\n",
    "    Testing data text.\n",
    "\n",
    "  model: Model\n",
    "    Trained keras model.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  y_test_pred: list\n",
    "    predicted labels for X_test.\n",
    "  \"\"\"\n",
    "  \n",
    "  norm_text = ArabicTextNormalizer().transform(X_test)\n",
    "  text_tkn = tokenizer.texts_to_sequences(norm_text)\n",
    "  text_tkn = pad_sequences(text_tkn, maxlen=50)\n",
    "\n",
    "  y_test_pred = np.argmax(model.predict(text_tkn),axis=1)\n",
    "  return [dialect_dict[i] for i in y_test_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Deep_learning_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding (Embedding)       (None, 50, 100)           10000000  \n",
      "                                                                 \n",
      " Dropout (SpatialDropout1D)  (None, 50, 100)           0         \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 18)                1818      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,082,218\n",
      "Trainable params: 10,082,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "                Embedding(100000, 100, input_length=50, mask_zero=True, name=\"Embedding\"),\n",
    "                SpatialDropout1D(0.2, name=\"Dropout\"),\n",
    "                LSTM(100, dropout=0.2, recurrent_dropout=0.2, name=\"LSTM\"), \n",
    "                Dense(18, activation='softmax', name=\"Dense\")\n",
    "                ], \n",
    "                    name=\"Deep_learning_Model\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1384/1384 [==============================] - 804s 576ms/step - loss: 1.9440 - accuracy: 0.3829 - val_loss: 1.6138 - val_accuracy: 0.4950\n",
      "Epoch 2/10\n",
      "1384/1384 [==============================] - 870s 628ms/step - loss: 1.3929 - accuracy: 0.5632 - val_loss: 1.5397 - val_accuracy: 0.5241\n",
      "Epoch 3/10\n",
      "1384/1384 [==============================] - 877s 633ms/step - loss: 1.1363 - accuracy: 0.6465 - val_loss: 1.5656 - val_accuracy: 0.5262\n"
     ]
    }
   ],
   "source": [
    "history = dl_fit(X_train, y_train, X_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Testing set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AE     0.4433    0.3867    0.4131      2630\n",
      "          BH     0.3221    0.3549    0.3377      2629\n",
      "          DZ     0.6041    0.4716    0.5297      1618\n",
      "          EG     0.6958    0.8334    0.7584      5764\n",
      "          IQ     0.5872    0.5342    0.5595      1550\n",
      "          JO     0.4577    0.2987    0.3615      2792\n",
      "          KW     0.4433    0.5946    0.5080      4211\n",
      "          LB     0.5770    0.6647    0.6178      2762\n",
      "          LY     0.5969    0.7036    0.6459      3650\n",
      "          MA     0.7097    0.5763    0.6361      1154\n",
      "          OM     0.4079    0.3719    0.3891      1912\n",
      "          PL     0.4799    0.5393    0.5079      4374\n",
      "          QA     0.4580    0.4831    0.4702      3107\n",
      "          SA     0.4192    0.3463    0.3793      2683\n",
      "          SD     0.7317    0.5690    0.6402      1443\n",
      "          SY     0.4212    0.3011    0.3512      1624\n",
      "          TN     0.6890    0.3669    0.4788       924\n",
      "          YE     0.4145    0.0977    0.1581       993\n",
      "\n",
      "    accuracy                         0.5237     45820\n",
      "   macro avg     0.5255    0.4719    0.4857     45820\n",
      "weighted avg     0.5208    0.5237    0.5140     45820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = dl_predict(X_test, model)\n",
    "report(\"Testing\", y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred,\n",
    "                                        cmap = \"cividis\", \n",
    "                                        xticks_rotation = \"vertical\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec26fe652b81aa16e73f5f2489daa0bd9f355124f013eb8138d239f76af89ec7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
